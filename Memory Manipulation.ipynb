{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9d435bae-c85f-4368-8cd5-0eff0928458e",
      "metadata": {
        "id": "9d435bae-c85f-4368-8cd5-0eff0928458e"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bba3631e-5016-40f4-bd46-cc91e7509f3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bba3631e-5016-40f4-bd46-cc91e7509f3e",
        "outputId": "db721b7e-9a78-4a3b-fb38-bbce630c6df1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: Ninja in /usr/local/lib/python3.10/dist-packages (1.11.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wurlitzer\n",
        "!pip install Ninja\n",
        "import os,math,sys,torch,re,numpy as np\n",
        "from types import SimpleNamespace as ns\n",
        "from collections import namedtuple\n",
        "# from utils import show_img,load_cuda,cuda_begin,cdiv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iE4REmIYKc1Y",
      "metadata": {
        "id": "iE4REmIYKc1Y"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "KPYLNWP2Zftn",
      "metadata": {
        "id": "KPYLNWP2Zftn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "\n",
        "import os,math,sys,torch,re,numpy as np\n",
        "from types import SimpleNamespace as ns\n",
        "from collections import namedtuple\n",
        "\n",
        "np.set_printoptions(precision=2, linewidth=140)\n",
        "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
        "\n",
        "def show_img(x, figsize=(4,3), **kwargs):\n",
        "    \"Display HW or CHW format image `x`\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis('off')\n",
        "    if len(x.shape)==3: x = x.permute(1,2,0)  # CHW -> HWC\n",
        "    plt.imshow(x.cpu(), **kwargs)\n",
        "\n",
        "cuda_begin = r'''\n",
        "#include <torch/extension.h>\n",
        "#include <stdio.h>\n",
        "#include <c10/cuda/CUDAException.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
        "#define CUDA_ERR(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess)\n",
        "   {\n",
        "      fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) exit(code);\n",
        "   }\n",
        "}\n",
        "__host__ __device__ inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a+b-1)/b;}\n",
        "'''\n",
        "\n",
        "def load_cuda(cuda_src, cpp_src, funcs, opt=True, verbose=False, name=None):\n",
        "    \"Simple wrapper for torch.utils.cpp_extension.load_inline\"\n",
        "    if name is None: name = funcs[0]\n",
        "    # flags = \"-O3 -Xptxas -O3 -Xcompiler -O3\" if opt else \"-O0 -Xptxas -O0 -Xcompiler -O0\"\n",
        "    return load_inline(cuda_sources=[cuda_src], cpp_sources=[cpp_src], functions=funcs, verbose=verbose, name=name)\n",
        "\n",
        "def cdiv(a,b):\n",
        "    \"Int ceiling division of `a` over `b`\"\n",
        "    return (a+b-1)//b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "V7wlHElmLzoV",
      "metadata": {
        "id": "V7wlHElmLzoV"
      },
      "outputs": [],
      "source": [
        "%load_ext wurlitzer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_xiJm91DL47I",
      "metadata": {
        "id": "_xiJm91DL47I"
      },
      "source": [
        "## Python Version in CUDA format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "J7EqszwHaSLz",
      "metadata": {
        "id": "J7EqszwHaSLz"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "dim3 = namedtuple('dim3', ['x','y','z'], defaults=(1,1))\n",
        "d = dim3(2,3)\n",
        "m1 = torch.rand(5120, 256)\n",
        "m1s = m1[:4]\n",
        "m2 = torch.rand(256,5120)\n",
        "m2s = m2[:,:4]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bbda41fd-dbbf-47d4-807a-67ad565b3bc8",
      "metadata": {
        "id": "bbda41fd-dbbf-47d4-807a-67ad565b3bc8"
      },
      "outputs": [],
      "source": [
        "# Functions\n",
        "def iterate_kerenel(f, blocks, threads, *args):\n",
        "    for i0 in range(blocks.y):\n",
        "        for i1 in range(blocks.x):\n",
        "            for j0 in range(threads.y):\n",
        "                for j1 in range(threads.x): f(dim3(i1,i0), dim3(j1,j0), threads, *args)\n",
        "def get_sig(fname, src):\n",
        "    res = re.findall(rf'^(.+\\s+{fname}\\(.*?\\))\\s*{{?\\s*$', src, re.MULTILINE)\n",
        "    return res[0]+';' if res else None\n",
        "\n",
        "def matmul_kernel(blockIdx, threadIdx, blockDim, m, n, out, h, w, k):\n",
        "    r = blockIdx.y*blockDim.y + threadIdx.y\n",
        "    c = blockIdx.x*blockDim.x + threadIdx.x\n",
        "\n",
        "    # boundary checking\n",
        "    if (r>=h or c>=w):\n",
        "       return\n",
        "\n",
        "    # matrix multiplication loop over flattened tensors\n",
        "    o = 0.0\n",
        "    for i in range(k):\n",
        "      o += m[r*k+i] * n[i*w+c]\n",
        "    out[r*w+c] = o\n",
        "\n",
        "\n",
        "\n",
        "def matmul(m, n):\n",
        "    h,k  = m.shape\n",
        "    k2,w = n.shape\n",
        "    assert k==k2, \"Size mismatch!\"\n",
        "    output = torch.zeros(h, w, dtype=m.dtype)\n",
        "    tpb = dim3(16,16)\n",
        "    blocks = dim3(cdiv(w,tpb.x), cdiv(h,tpb.y))\n",
        "    iterate_kerenel(matmul_kernel, blocks, tpb,\n",
        "                 m.flatten(), n.flatten(), output.flatten(), h, w, k)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5b57350f-3ff6-4e10-8635-040c3736220d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b57350f-3ff6-4e10-8635-040c3736220d",
        "outputId": "172c5f3a-e8bd-405d-f545-bbcdc4b7af17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test\n",
        "torch.isclose(matmul(m1s, m2s), m1s@m2s).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yiZ-53_CMdz7",
      "metadata": {
        "id": "yiZ-53_CMdz7"
      },
      "source": [
        "## CUDA version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "fbSzJz_lMu6V",
      "metadata": {
        "id": "fbSzJz_lMu6V"
      },
      "outputs": [],
      "source": [
        "cuda_src = cuda_begin + r'''\n",
        "__global__ void matmul_k(float* m, float* n, float* out, int h, int w, int k) {\n",
        "    int r = blockIdx.y*blockDim.y + threadIdx.y;\n",
        "    int c = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (r>=h || c>=w) return;\n",
        "    float o = 0;\n",
        "    for (int i = 0; i<k; ++i) o += m[r*k+i] * n[i*w+c];\n",
        "    out[r*w+c] = o;\n",
        "}\n",
        "\n",
        "torch::Tensor matmul(torch::Tensor m, torch::Tensor n) {\n",
        "    CHECK_INPUT(m); CHECK_INPUT(n);\n",
        "    int h = m.size(0);\n",
        "    int w = n.size(1);\n",
        "    int k = m.size(1);\n",
        "    TORCH_CHECK(k==n.size(0), \"Size mismatch!\");\n",
        "    auto output = torch::zeros({h, w}, m.options());\n",
        "\n",
        "    dim3 tpb(16,16);\n",
        "    dim3 blocks(cdiv(w, tpb.x), cdiv(h, tpb.y));\n",
        "    matmul_k<<<blocks, tpb>>>(\n",
        "        m.data_ptr<float>(), n.data_ptr<float>(), output.data_ptr<float>(), h, w, k);\n",
        "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "    return output;\n",
        "}\n",
        "'''\n",
        "fname = 'matmul'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "WIOF1vqha-D0",
      "metadata": {
        "id": "WIOF1vqha-D0"
      },
      "outputs": [],
      "source": [
        "cpp_src = get_sig(fname, cuda_src)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "zOTf0zfWcO_M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOTf0zfWcO_M",
        "outputId": "c8bb4f9f-8d26-492e-a0c0-da042bee549a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch::Tensor matmul(torch::Tensor m, torch::Tensor n);\n"
          ]
        }
      ],
      "source": [
        "print(cpp_src)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "TVq_yyiybGxN",
      "metadata": {
        "id": "TVq_yyiybGxN"
      },
      "outputs": [],
      "source": [
        "module = load_cuda(cuda_src, cpp_src, [fname])\n",
        "m1c,m2c = m1.contiguous().cuda(),m2.contiguous().cuda()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "eHMI7YMrc_6j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHMI7YMrc_6j",
        "outputId": "8f06c581-c80f-4a98-b16c-0fb1293d8477"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(True, device='cuda:0')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "module.matmul(m1c,m2c).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "tkb2KbqbbVic",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkb2KbqbbVic",
        "outputId": "f1b7ac75-46a7-4821-ef0a-4a87a8b20170"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(True, device='cuda:0')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.isclose(module.matmul(m1c,m2c), m1c@m2c).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OMp89DB_dKzm",
      "metadata": {
        "id": "OMp89DB_dKzm"
      },
      "source": [
        "## Memory Tiling (Python)\n",
        "Python version of memory tiling to make the CUDA code more efficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "0aIZSbFZdHT7",
      "metadata": {
        "id": "0aIZSbFZdHT7"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "from threading import Barrier, Thread\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "# The Kernel\n",
        "def matmul_tiled_bk(blockIdx, threadIdx, blockDim, shared, syncb, m, n, out, h, w, k, tw):\n",
        "    tc,tr = threadIdx.x,threadIdx.y\n",
        "    r = blockIdx.y*blockDim.y + tr\n",
        "    c = blockIdx.x*blockDim.x + tc\n",
        "\n",
        "    shar_sz = tw*tw\n",
        "    ms,ns = shared[:shar_sz],shared[shar_sz:]\n",
        "\n",
        "    p = 0.0\n",
        "    for ph in range(cdiv(k,tw)):\n",
        "        # Calculate the shared memory\n",
        "        ms[tr*tw+tc] = m[ tc + ph*tw + r*k] if r<h and (ph*tw+tc)<k else 0.\n",
        "        ns[tr*tw+tc] = n[(tr + ph*tw)*w +c] if c<w and (ph*tw+tr)<k else 0.\n",
        "\n",
        "        # Sync up with other threads in block\n",
        "        syncb.wait()\n",
        "\n",
        "        # Utilize shared memory\n",
        "        for i in range(tw): p += ms[tr*tw+i] * ns[tw*i+tc]\n",
        "        syncb.wait()\n",
        "\n",
        "    if (r<h and c<w): out[r*w + c] = p\n",
        "\n",
        "\n",
        "# Simulates CUDA scheduling / processing\n",
        "def blk_kernel2d_shar(f, blocks, tpb, sh_sz, *args, **kwargs):\n",
        "    for i0 in range(blocks.y):\n",
        "        for i1 in range(blocks.x):\n",
        "            shar = torch.zeros(sh_sz)\n",
        "            syncb = Barrier(tpb.y*tpb.x)\n",
        "\n",
        "            # Create threads\n",
        "            threads = [Thread(target=f, args=(dim3(i1,i0), dim3(p,o), tpb, shar, syncb, *args), kwargs=kwargs)\n",
        "                       for o in range(tpb.y) for p in range(tpb.x)]\n",
        "            for tr in threads: tr.start()\n",
        "            for tr in threads: tr.join()\n",
        "\n",
        "\n",
        "# Matrix multiplication\n",
        "def matmul_2d(m, n, tw=16):\n",
        "    h,k  = m.shape\n",
        "    k2,w = n.shape\n",
        "    assert k==k2, \"Size mismatch!\"\n",
        "    output = torch.zeros(h, w, dtype=m.dtype)\n",
        "    tpb = dim3(tw,tw)\n",
        "    blocks = dim3(cdiv(w,tpb.x), cdiv(h,tpb.y))\n",
        "\n",
        "    # Launch the kernel with the arguments\n",
        "    blk_kernel2d_shar(matmul_tiled_bk, blocks, tpb, tw*tw*2,\n",
        "                      m.flatten(), n.flatten(), output.flatten(),\n",
        "                      h, w, k, tw=tw)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "t0h8gJbtfW9m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0h8gJbtfW9m",
        "outputId": "9e743718-98e0-4409-ca79-4a9111bacaae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.isclose(matmul_2d(m1s, m2s, tw=8), m1s@m2s).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SuDfz1xbfl7w",
      "metadata": {
        "id": "SuDfz1xbfl7w"
      },
      "source": [
        "## CUDA using tiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "CZC47z6nfY4j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CZC47z6nfY4j",
        "outputId": "e780205c-0a94-4238-9068-b6ff465b9077"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'torch::Tensor matmul_grid(torch::Tensor m, torch::Tensor n);'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cuda_src = cuda_begin + r'''\n",
        "__global__ void matmul_tiled_bk(float* m, float* n, float* out, int h, int w, int k, int tw) {\n",
        "    int tc = threadIdx.x;\n",
        "    int tr = threadIdx.y;\n",
        "    int r = blockIdx.y * blockDim.y + tr;\n",
        "    int c = blockIdx.x * blockDim.x + tc;\n",
        "\n",
        "    // Load shared (within block) memoryh\n",
        "    extern __shared__ float shared[];\n",
        "\n",
        "    float* ms = &shared[0];\n",
        "    float* ns = &shared[tw * tw];\n",
        "\n",
        "    float p = 0.0;\n",
        "    for (int ph = 0; ph < cdiv(k, tw); ++ph) {\n",
        "\n",
        "        // Calculate the shared memory\n",
        "        ms[tr * tw + tc] = (r < h && (ph * tw + tc) < k) ? m[tc + ph * tw + r * k] : 0.0;\n",
        "        ns[tr * tw + tc] = (c < w && (ph * tw + tr) < k) ? n[(tr + ph * tw) * w + c] : 0.0;\n",
        "\n",
        "        // Sync up with other threads in block\n",
        "        __syncthreads();\n",
        "\n",
        "        // Utilize shared memory\n",
        "        for (int i = 0; i < tw; ++i) p += ms[tr * tw + i] * ns[tw * i + tc];\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (r < h && c < w) out[r * w + c] = p;\n",
        "}\n",
        "\n",
        "torch::Tensor matmul_grid(torch::Tensor m, torch::Tensor n) {\n",
        "    CHECK_INPUT(m); CHECK_INPUT(n);\n",
        "    int h=m.size(0), w=n.size(1), k=m.size(1);\n",
        "    TORCH_CHECK(k==n.size(0), \"Size mismatch!\");\n",
        "\n",
        "    auto output = torch::zeros({h, w}, m.options());\n",
        "    int TW = 16;\n",
        "    size_t size = TW*TW * 2 * sizeof(float);\n",
        "    dim3 tpb(TW,TW);\n",
        "    dim3 blocks(cdiv(w, tpb.x), cdiv(h, tpb.y));\n",
        "    matmul_tiled_bk<<<blocks,tpb,size>>>(\n",
        "        m.data_ptr<float>(), n.data_ptr<float>(), output.data_ptr<float>(), h, w, k, TW);\n",
        "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "\n",
        "\n",
        "    return output;\n",
        "}\n",
        "'''\n",
        "fun = \"matmul_grid\"\n",
        "cpp_src = get_sig(fun, cuda_src)\n",
        "cpp_src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "dEtLtFeEglI8",
      "metadata": {
        "id": "dEtLtFeEglI8"
      },
      "outputs": [],
      "source": [
        "module = load_cuda(cuda_src, cpp_src, [fun], opt=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "-mWm9La1gzBe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mWm9La1gzBe",
        "outputId": "37ecc2e4-1e90-4f06-ae8d-d0bd6a062a81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(True, device='cuda:0')"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.isclose(module.matmul_grid(m1c,m2c), m1c@m2c).all()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "uV5Q_RFkg4Ly",
      "metadata": {
        "id": "uV5Q_RFkg4Ly"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "83hJ30wog5XE",
      "metadata": {
        "id": "83hJ30wog5XE"
      },
      "outputs": [],
      "source": [
        "# Numba seems to make the whole multithreading process much easier to simulate in raw Python, but different syntax\n",
        "\n",
        "from numba import cuda\n",
        "from numba.cuda import as_cuda_array as ca\n",
        "\n",
        "@cuda.jit\n",
        "def matmul_k_numba(m, n, out, tw):\n",
        "    cbi,cbd,tid = cuda.blockIdx,cuda.blockDim,cuda.threadIdx\n",
        "    tc,tr = tid.x,tid.y\n",
        "    r,c = cbi.y * cbd.y + tr, cbi.x * cbd.x + tc\n",
        "    h,k  = m.shape\n",
        "    k2,w = n.shape\n",
        "\n",
        "    # Load shared array\n",
        "    shar = cuda.shared.array(0, dtype=np.float32)\n",
        "    ms,ns = shar[:tw*tw],shar[tw*tw:2*tw*tw]\n",
        "\n",
        "    p = np.float32(0.0)\n",
        "    for ph in range(math.ceil(k/tw)):\n",
        "        idx = ph*tw\n",
        "        ms[tr*tw+tc] = m[r, tc+idx] if r<h and idx+tc<k else 0.\n",
        "        ns[tr*tw+tc] = n[tr+idx, c] if c<w and idx+tr<k else 0.\n",
        "        cuda.syncthreads()\n",
        "\n",
        "\n",
        "        for i in range(tw):\n",
        "          p += ms[tr*tw+i] * ns[i*tw+tc]\n",
        "        cuda.syncthreads()\n",
        "\n",
        "    if r < h and c < w:\n",
        "      out[r, c] = p\n",
        "\n",
        "def matmul_2d_numba(m, n, tw=16):\n",
        "    h,k  = m.shape\n",
        "    k2,w = n.shape\n",
        "    assert k==k2, \"Size mismatch!\"\n",
        "    out = torch.zeros(h, w, dtype=m.dtype, device=m.device)\n",
        "    dyn_shared_mem_size = 2 * tw * tw * 4\n",
        "    tpb = tw,tw\n",
        "    blocks = cdiv(w,tpb[0]), cdiv(h,tpb[1])\n",
        "    matmul_k_numba[blocks, tpb, 0, dyn_shared_mem_size](ca(m), ca(n), ca(out), tw)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "3r28VeCeiQI7",
      "metadata": {
        "id": "3r28VeCeiQI7"
      },
      "outputs": [],
      "source": [
        "matmul_2d_numba(m1c,m2c)\n",
        "torch.cuda.synchronize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "KFOhhRuwiy_h",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFOhhRuwiy_h",
        "outputId": "3204584e-0878-476f-ae33-e7245e00e3a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(True, device='cuda:0')"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.isclose(matmul_2d_numba(m1c,m2c), m1c@m2c).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VczHNC_8i5rk",
      "metadata": {
        "id": "VczHNC_8i5rk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
