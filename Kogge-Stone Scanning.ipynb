{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9d435bae-c85f-4368-8cd5-0eff0928458e",
      "metadata": {
        "id": "9d435bae-c85f-4368-8cd5-0eff0928458e"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba3631e-5016-40f4-bd46-cc91e7509f3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bba3631e-5016-40f4-bd46-cc91e7509f3e",
        "outputId": "dee294f0-2f62-4d12-d991-3a295ec3d664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: Ninja in /usr/local/lib/python3.10/dist-packages (1.11.1.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (0.3.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install wurlitzer\n",
        "!pip install Ninja\n",
        "!pip install dill\n",
        "import os,math,sys,torch,re,numpy as np\n",
        "from types import SimpleNamespace as ns\n",
        "from collections import namedtuple\n",
        "# from utils import show_img,load_cuda,cuda_begin,cdiv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iE4REmIYKc1Y",
      "metadata": {
        "id": "iE4REmIYKc1Y"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KPYLNWP2Zftn",
      "metadata": {
        "id": "KPYLNWP2Zftn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "\n",
        "import os,math,sys,torch,re,numpy as np\n",
        "from types import SimpleNamespace as ns\n",
        "from collections import namedtuple\n",
        "\n",
        "np.set_printoptions(precision=2, linewidth=140)\n",
        "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
        "\n",
        "def show_img(x, figsize=(4,3), **kwargs):\n",
        "    \"Display HW or CHW format image `x`\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis('off')\n",
        "    if len(x.shape)==3: x = x.permute(1,2,0)  # CHW -> HWC\n",
        "    plt.imshow(x.cpu(), **kwargs)\n",
        "\n",
        "cuda_begin = r'''\n",
        "#include <torch/extension.h>\n",
        "#include <stdio.h>\n",
        "#include <c10/cuda/CUDAException.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
        "#define CUDA_ERR(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess)\n",
        "   {\n",
        "      fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) exit(code);\n",
        "   }\n",
        "}\n",
        "__host__ __device__ inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a+b-1)/b;}\n",
        "'''\n",
        "\n",
        "def load_cuda(cuda_src, cpp_src, funcs, opt=True, verbose=False, name=None):\n",
        "    \"Simple wrapper for torch.utils.cpp_extension.load_inline\"\n",
        "    if name is None: name = funcs[0]\n",
        "    # flags = \"-O3 -Xptxas -O3 -Xcompiler -O3\" if opt else \"-O0 -Xptxas -O0 -Xcompiler -O0\"\n",
        "    return load_inline(cuda_sources=[cuda_src], cpp_sources=[cpp_src], functions=funcs, verbose=verbose, name=name)\n",
        "\n",
        "def cdiv(a,b):\n",
        "    \"Int ceiling division of `a` over `b`\"\n",
        "    return (a+b-1)//b\n",
        "\n",
        "def get_sig(fname, src):\n",
        "    res = re.findall(rf'^(.+\\s+{fname}\\(.*?\\))\\s*{{?\\s*$', src, re.MULTILINE)\n",
        "    return res[0]+';' if res else None\n",
        "\n",
        "dim3 = namedtuple('dim3', ['x','y','z'], defaults=(1,1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V7wlHElmLzoV",
      "metadata": {
        "id": "V7wlHElmLzoV"
      },
      "outputs": [],
      "source": [
        "%load_ext wurlitzer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_xiJm91DL47I",
      "metadata": {
        "id": "_xiJm91DL47I"
      },
      "source": [
        "# Basic Parallel Approach\n",
        "## Numba Version in CUDA format\n",
        "\n",
        "The general idea of the Brent-Kung approach to scanning is as follows\n",
        "\n",
        "First, the **reduction phase**\n",
        "- Assign one thread to every 2 elements\n",
        "- Have each element perform a scan with the index its assigned and with the element that is 2^t steps prior to it (t=timestep)\n",
        "- Repeat as long as there are indices which can perform the operation\n",
        "\n",
        "You'll now have an array where each of the elements that belong to an index that is a power of 2 have succesfully completed the scan. But hte others will be in various states of disarray. Thus, you should perform the **post reduction phase**.\n",
        "\n",
        "This impelmentation, however, is the Kogge-Stone Algorithm, which essentially\n",
        "- Assigns each thread to a specific index\n",
        "- Then, loops through and assigns to each index the sum of itself + the element 2**t prior\n",
        "Each index = sum of self + previous element\n",
        "- Once done- store the final indexâ€™s value as partial sum (global memory)\n",
        "- Then, distributes the partial sums\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "from numba.cuda import as_cuda_array as ca\n",
        "\n",
        "# Let's assume addition\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def kogge_kernel(m, out, partials):\n",
        "    cbi,cbd,tid = cuda.blockIdx,cuda.blockDim,cuda.threadIdx\n",
        "    w = m.shape[0]\n",
        "\n",
        "    # thread's index\n",
        "    r = cbi.x * cbd.x + tid.x\n",
        "    # Initialize shared memory\n",
        "    shared_mem = cuda.shared.array(shape=(16,), dtype=np.float32)\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    shared_mem[r] = m[r]\n",
        "\n",
        "    for i in range(12):\n",
        "        temp = 0\n",
        "        if (2**i<=r):\n",
        "          temp = shared_mem[r-2**i] + shared_mem[r]\n",
        "        cuda.syncthreads()\n",
        "\n",
        "        if (2**i<=r):\n",
        "          shared_mem[r] = temp\n",
        "        cuda.syncthreads()\n",
        "\n",
        "    out[r]=shared_mem[r]\n",
        "    if (cuda.threadIdx.x==cuda.blockDim.x-1):\n",
        "      partials[cbi.x]=shared_mem[r]\n",
        "\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def kogge_add(out, partials):\n",
        "    cbi, cbd, tid = cuda.blockIdx, cuda.blockDim, cuda.threadIdx\n",
        "    w = out.shape[0]\n",
        "    r = cbi.x * cbd.x + tid.x\n",
        "\n",
        "    # Each thread in a block adds the partial sum from the previous block\n",
        "    if cbi.x > 0 and r < w:\n",
        "        out[r] += partials[cbi.x - 1]\n",
        "\n",
        "def run_kogge(m, tw=16):\n",
        "\n",
        "    w  = m.shape[0]\n",
        "    out = torch.zeros(w, dtype=m.dtype, device=m.device)\n",
        "\n",
        "    block_no = int(w/tw)\n",
        "    partials = torch.zeros(block_no, dtype=m.dtype, device=m.device)\n",
        "\n",
        "    # Get partial sums + do scans over individaul blocks\n",
        "    kogge_kernel[block_no, tw](ca(m), ca(out), ca(partials))\n",
        "\n",
        "    # tally up partial sums\n",
        "    for i in range(1,len(partials)):\n",
        "      partials[i]+=partials[i-1]\n",
        "\n",
        "    # Distribute on GPU\n",
        "    if block_no>1:\n",
        "      kogge_add[block_no, tw](ca(out), ca(partials))\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "yQ8sgqpAUwip"
      },
      "id": "yQ8sgqpAUwip",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.ones([64])\n",
        "run_kogge(input.to(\"cuda\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLamGWjzWSQl",
        "outputId": "b00284de-5bda-4dde-d752-b79b812faefa"
      },
      "id": "fLamGWjzWSQl",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26.,\n",
              "        27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52.,\n",
              "        53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64.], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iPT0dxkqWSS7"
      },
      "id": "iPT0dxkqWSS7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y08qaCE-WSVQ"
      },
      "id": "y08qaCE-WSVQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_kogge(d=)"
      ],
      "metadata": {
        "id": "hX-yhQz8WQpG"
      },
      "id": "hX-yhQz8WQpG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J7EqszwHaSLz",
      "metadata": {
        "id": "J7EqszwHaSLz"
      },
      "outputs": [],
      "source": [
        "from numba import cuda\n",
        "from numba.cuda import as_cuda_array as ca\n",
        "\n",
        "# Let's assume addition\n",
        "\n",
        "@cuda.jit\n",
        "def numba_kernel_brent(m, out):\n",
        "    cbi,cbd,tid = cuda.blockIdx,cuda.blockDim,cuda.threadIdx\n",
        "    tc,tr = tid.x,tid.y\n",
        "    r = tid.x * 2\n",
        "    w = m.shape[0]\n",
        "\n",
        "    # Loading step\n",
        "    out[r+1] = m[r+1]\n",
        "    out[r] = m[r]\n",
        "\n",
        "    for i in range(w):\n",
        "        temp = 0\n",
        "        if (2^i<=r):\n",
        "          temp = out[r-2^i] + out[r]\n",
        "        cuda.syncthreads()\n",
        "    #     # Need 2 conditionals- can't sync threads within a divergent passway\n",
        "        if (2^i<=r):\n",
        "          out[r] = temp\n",
        "        cuda.syncthreads()\n",
        "    # return\n",
        "\n",
        "\n",
        "# cuda.syncthreads()\n",
        "\n",
        "\n",
        "\n",
        "def run_brent(m, tw=16):\n",
        "    w  = m.shape[0]\n",
        "    out = torch.zeros(w, dtype=m.dtype, device=m.device)\n",
        "\n",
        "    numba_kernel_brent[1, int(w/2), 0](ca(m), ca(out))\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbda41fd-dbbf-47d4-807a-67ad565b3bc8",
      "metadata": {
        "id": "bbda41fd-dbbf-47d4-807a-67ad565b3bc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc75cf19-36e7-43ea-9d4c-f3301286cb09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "\n",
        "input = torch.ones(12)\n",
        "input.unsqueeze(0)\n",
        "# input = torch.array(input)\n",
        "input.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiPuUSiXeIWP",
        "outputId": "d99ffa54-3007-41b9-9a01-b12e696cf8bb"
      },
      "id": "kiPuUSiXeIWP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "run_brent(input.to(\"cuda\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGOxe2UpKLL7",
        "outputId": "1b5b186a-8e4d-4846-fa80-fd14616d3ce2"
      },
      "id": "JGOxe2UpKLL7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 4.00,  1.00,  5.00,  1.00, 10.00,  1.00, 17.00,  1.00, 18.00,  1.00, 20.75,  1.00], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VczHNC_8i5rk",
      "metadata": {
        "id": "VczHNC_8i5rk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "fae588fa-c3dc-4d73-c961-2e9668a23c3f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'convolution_22' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-d50c211d83fa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolution_22\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'convolution_22' is not defined"
          ]
        }
      ],
      "source": [
        "out = convolution_22(input)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numba Version"
      ],
      "metadata": {
        "id": "AXhOVK_KmWh6"
      },
      "id": "AXhOVK_KmWh6"
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "from numba.cuda import as_cuda_array as ca\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def cuda_numba(m, out):\n",
        "    cbi,cbd,tid = cuda.blockIdx,cuda.blockDim,cuda.threadIdx\n",
        "    tc,tr = tid.x,tid.y\n",
        "    r, c = cbi.y * cbd.y + tr, cbi.x * cbd.x + tc\n",
        "    h, w = m.shape[0], m.shape[1]\n",
        "    index = r*w + c\n",
        "    c_out = 0\n",
        "    for x in range(max(0, r - 1), min(r + 2, h)):\n",
        "      for y in range(max(0, c - 1), min(c + 2, w)):\n",
        "          c_out += m[x, y]\n",
        "    out[r, c] = c_out\n",
        "\n",
        "\n",
        "def conv_cuda(m):\n",
        "\n",
        "    w,h = m.shape\n",
        "    output = torch.zeros(w,h, dtype=m.dtype, device=m.device)\n",
        "    thread_block = dim3(3, 3)\n",
        "    log = np.zeros(w * h, dtype=np.int32)  # Create a log array\n",
        "\n",
        "    # Calculate the number of blocks needed\n",
        "    blocks_x = math.ceil(w / thread_block.x)\n",
        "    blocks_y = math.ceil(h / thread_block.y)\n",
        "    blocks = blocks_x, blocks_y\n",
        "\n",
        "    # Create the output tensor\n",
        "\n",
        "    cuda_numba[blocks, (3,3)](ca(m), ca(output))\n",
        "\n",
        "\n",
        "    return output\n",
        "\n",
        "conv_cuda(torch.ones(12,12).to(\"cuda\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z7lEDqImYBf",
        "outputId": "64d0c8ad-eb46-4d42-ca8f-d223f14048af"
      },
      "id": "_Z7lEDqImYBf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 4.],\n",
              "        [6., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 6.],\n",
              "        [6., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 6.],\n",
              "        [6., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 6.],\n",
              "        [6., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 6.],\n",
              "        [6., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 6.],\n",
              "        [6., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 6.],\n",
              "        [6., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 6.],\n",
              "        [6., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 6.],\n",
              "        [6., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 6.],\n",
              "        [6., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 6.],\n",
              "        [4., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 4.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA Version"
      ],
      "metadata": {
        "id": "paYJqKTNLqmu"
      },
      "id": "paYJqKTNLqmu"
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_src = cuda_begin + r'''\n",
        "__constant__ float c_M[9] = {1, 1, 1, 1, 1, 1, 1, 1, 1};\n",
        "\n",
        "__device__ int two_to_one(int r, int c, int c_size) {\n",
        "    return r * c_size + c;  // Adjusted for correct row-major indexing\n",
        "}\n",
        "\n",
        "__global__ void conv_kernel(const float* m, float* out, int w, int h) {\n",
        "    int r = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int c = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (r < w && c < h) {\n",
        "        int index = two_to_one(r, c, h);\n",
        "\n",
        "        float c_out = 0.0f;\n",
        "        int maskIndex = 0;\n",
        "\n",
        "        for (int x = r - 1; x <= r + 1; x++) {\n",
        "            for (int y = c - 1; y <= c + 1; y++) {\n",
        "                if (x >= 0 && x < w && y >= 0 && y < h) {\n",
        "                    c_out += m[two_to_one(x, y, h)] * c_M[maskIndex];\n",
        "                }\n",
        "                maskIndex++;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        out[index] = c_out;\n",
        "    }\n",
        "}\n",
        "'''\n",
        "\n",
        "cuda_src += r'''\n",
        "torch::Tensor convolution_22(torch::Tensor m) {\n",
        "    int w = m.size(0);\n",
        "    int h = m.size(1);\n",
        "\n",
        "    auto options = torch::TensorOptions().dtype(m.dtype()).device(m.device());\n",
        "    torch::Tensor output = torch::zeros({w, h}, options);\n",
        "\n",
        "    dim3 thread_block(3, 3);\n",
        "    int blocks_x = std::ceil(static_cast<float>(w) / thread_block.x);\n",
        "    int blocks_y = std::ceil(static_cast<float>(h) / thread_block.y);\n",
        "    dim3 blocks(blocks_x, blocks_y);\n",
        "\n",
        "    conv_kernel<<<blocks, thread_block>>>(m.data_ptr<float>(), output.data_ptr<float>(), w, h);\n",
        "\n",
        "    // Wait for the GPU to finish and check for any errors\n",
        "    cudaDeviceSynchronize();\n",
        "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "\n",
        "    return output;\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "diTTqxB4MOeP"
      },
      "id": "diTTqxB4MOeP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname = \"convolution_22\"\n",
        "cpp_src = get_sig(fname, cuda_src)\n",
        "cpp_src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Cc5sjAgoM5ol",
        "outputId": "751f9b68-6b7a-40fc-bc07-c450894822d4"
      },
      "id": "Cc5sjAgoM5ol",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch::Tensor convolution_22(torch::Tensor m);'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZKwSCminODvA"
      },
      "id": "ZKwSCminODvA"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "AXhOVK_KmWh6"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}